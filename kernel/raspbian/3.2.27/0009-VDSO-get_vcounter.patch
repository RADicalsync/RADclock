From 6fe4278deb7287690a2b6c9ca20d37cb592145c8 Mon Sep 17 00:00:00 2001
From: Julien Ridoux <julien@synclab.org>
Date: Mon, 30 Aug 2010 20:32:59 +1000
Subject: [PATCH 9/9] VDSO get_vcounter

Support for VDSO get_vcounter() for accessing the raw cumulative
counter from user space.
Changes to libc will be required to take advantage of it.
---
 arch/x86/include/asm/vgtod.h   |    4 ++
 arch/x86/kernel/vsyscall_64.c  |    4 ++
 arch/x86/vdso/vclock_gettime.c |   95 ++++++++++++++++++++++++++++++++++++++++
 3 files changed, 103 insertions(+), 0 deletions(-)

diff --git a/arch/x86/include/asm/vgtod.h b/arch/x86/include/asm/vgtod.h
index 815285b..7524e5c1a 100644
--- a/arch/x86/include/asm/vgtod.h
+++ b/arch/x86/include/asm/vgtod.h
@@ -16,10 +16,14 @@ struct vsyscall_gtod_data {
		int vclock_mode;
		cycle_t	cycle_last;
		cycle_t	mask;
		u32	mult;
		u32	shift;
+#ifdef CONFIG_RADCLOCK
+		vcounter_t vcounter_record;
+		vcounter_t vcounter_source_record;
+#endif
	} clock;
	struct timespec wall_to_monotonic;
	struct timespec wall_time_coarse;
 };
 extern struct vsyscall_gtod_data vsyscall_gtod_data;
diff --git a/arch/x86/kernel/vsyscall_64.c b/arch/x86/kernel/vsyscall_64.c
index e4d4a22..42962c8 100644
--- a/arch/x86/kernel/vsyscall_64.c
+++ b/arch/x86/kernel/vsyscall_64.c
@@ -103,10 +103,14 @@ void update_vsyscall(struct timespec *wall_time, struct timespec *wtm,
	vsyscall_gtod_data.clock.shift		= clock->shift;
	vsyscall_gtod_data.wall_time_sec	= wall_time->tv_sec;
	vsyscall_gtod_data.wall_time_nsec	= wall_time->tv_nsec;
	vsyscall_gtod_data.wall_to_monotonic	= *wtm;
	vsyscall_gtod_data.wall_time_coarse	= __current_kernel_time();
+#ifdef CONFIG_RADCLOCK
+	vsyscall_gtod_data.clock.vcounter_record = clock->vcounter_record;
+	vsyscall_gtod_data.clock.vcounter_source_record = clock->vcounter_source_record;
+#endif

	write_sequnlock_irqrestore(&vsyscall_gtod_data.lock, flags);
 }

 static void warn_bad_vsyscall(const char *level, struct pt_regs *regs,
diff --git a/arch/x86/vdso/vclock_gettime.c b/arch/x86/vdso/vclock_gettime.c
index 6bc0e72..85241c3 100644
--- a/arch/x86/vdso/vclock_gettime.c
+++ b/arch/x86/vdso/vclock_gettime.c
@@ -214,5 +214,100 @@ notrace time_t __vdso_time(time_t *t)
		*t = result;
	return result;
 }
 int time(time_t *t)
	__attribute__((weak, alias("__vdso_time")));
+
+#ifdef CONFIG_RADCLOCK
+/* Copy of the version in kernel/time/timekeeping.c which we cannot directly access */
+/* Only called while gtod->lock is held */
+notrace static inline vcounter_t vread_ffcounter_delta(void)
+{
+	if (gtod->clock.vclock_mode == VCLOCK_TSC)
+		return((vread_tsc() - gtod->clock.vcounter_source_record)
+				& gtod->clock.mask);
+	else
+		return((vread_hpet() - gtod->clock.vcounter_source_record)
+				& gtod->clock.mask);
+
+}
+
+/* Copy of the version in kernel/time/timekeeping.c which we cannot directly access */
+notrace static inline vcounter_t vread_ffcounter(void)
+{
+	unsigned long seq;
+	vcounter_t vcount;
+
+	do {
+		seq = read_seqbegin(&gtod->lock);
+		vcount = gtod->clock.vcounter_record + vread_ffcounter_delta();
+	} while (read_seqretry(&gtod->lock, seq));
+
+	return vcount;
+}
+
+notrace static long vdso_fallback_get_vcounter(vcounter_t *vcounter)
+{
+	long ret;
+	asm("syscall" : "=a" (ret) :
+	    "0" (__NR_get_vcounter), "D" (vcounter) : "memory");
+	return ret;
+}
+
+notrace int __vdso_get_vcounter(vcounter_t *vcounter)
+{
+	vcounter_t vcount;
+
+	if (likely(gtod->clock.vclock_mode != VCLOCK_NONE)) {
+		vcount = vread_ffcounter();
+		*vcounter = vcount;
+		return 0;
+	}
+	return vdso_fallback_get_vcounter(vcounter);
+}
+int get_vcounter(vcounter_t *)
+	__attribute__((weak, alias("__vdso_get_vcounter")));
+
+
+notrace int __vdso_get_vcounter_latency(vcounter_t *vcounter, cycle_t *vcount_lat, cycle_t *tsc_lat)
+{
+/* XEN paravirtualization does not seem to like the rdtscll call, and redefines
+ * it in parvirt.h. It is a bit dodgy but allow compilation ... and not used so
+ * far, it is more a record what should be done.
+ */
+#ifdef CONFIG_PARAVIRT
+#define real_rdtscll(val) (val = __native_read_tsc())
+#else
+#define real_rdtscll(val) rdtscll(val)
+#endif
+	vcounter_t vcount;
+	cycle_t tsc1, tsc2, tsc3;
+
+	long ret;
+
+	if (likely(gtod->clock.vclock_mode != VCLOCK_NONE)) {
+		/* One for fun and warmup */
+		real_rdtscll(tsc1);
+		__asm __volatile("lfence" ::: "memory");
+		real_rdtscll(tsc1);
+		__asm __volatile("lfence" ::: "memory");
+		real_rdtscll(tsc2);
+		__asm __volatile("lfence" ::: "memory");
+		vcount = vread_ffcounter();
+		__asm __volatile("lfence" ::: "memory");
+		real_rdtscll(tsc3);
+		__asm __volatile("lfence" ::: "memory");
+
+		*vcounter = vcount;
+		*vcount_lat = tsc3 - tsc2;
+		*tsc_lat = tsc2 - tsc1;
+
+		return 0;
+	}
+	asm("syscall" : "=a" (ret) :
+	    "0" (__NR_get_vcounter_latency), "D" (vcounter), "S" (vcount_lat), "q" (tsc_lat)  : "memory");
+	return ret;
+}
+long get_vcounter_latency(vcounter_t *, cycle_t *, cycle_t *)
+	__attribute__((weak, alias("__vdso_get_vcounter_latency")));
+
+#endif  /* CONFIG_RADCLOCK */
--
1.7.5.4
